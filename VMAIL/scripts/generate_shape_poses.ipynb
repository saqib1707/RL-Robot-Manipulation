{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import uuid\n",
    "import io\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import json\n",
    "import pprint\n",
    "import pathlib\n",
    "import open3d as o3d\n",
    "from tqdm import tqdm\n",
    "import colored_traceback\n",
    "colored_traceback.add_hook()\n",
    "\n",
    "import robosuite as suite\n",
    "import robosuite.utils.camera_utils as cam_utils\n",
    "from robosuite.wrappers import GymWrapper\n",
    "suite.macros.IMAGE_CONVENTION = \"opencv\"    # Set the image convention to opencv so the images are automatically rendered \"right side up\" when using imageio (which uses opencv convention)\n",
    "\n",
    "# Get the home directory of the current user\n",
    "home_dir = os.path.expanduser('~')\n",
    "sys.path.append(os.path.join(home_dir, \"Desktop/ERLab/robot_manipulation/SceneGrasp/\"))\n",
    "\n",
    "from common.utils.nocs_utils import load_depth\n",
    "from common.utils.misc_utils import convert_realsense_rgb_depth_to_o3d_pcl, get_o3d_pcd_from_np, get_scene_grasp_model_params\n",
    "from common.utils.scene_grasp_utils import SceneGraspModel, get_final_grasps_from_predictions_np, get_grasp_vis\n",
    "\n",
    "from robosuite_utils import load_episodes, create_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    \"\"\"\n",
    "    This class allows you to access python dictionary elements using dot \n",
    "    notation `obj.key` instead of the usual square brackets `obj['key']`.\n",
    "    \"\"\"\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __getattr__ = dict.__getitem__\n",
    "\n",
    "\n",
    "def args_type(default):\n",
    "    if isinstance(default, bool):\n",
    "        return lambda x: bool(['False', 'True'].index(x))\n",
    "    if isinstance(default, int):\n",
    "        return lambda x: float(x) if ('e' in x or '.' in x) else int(x)\n",
    "    if isinstance(default, pathlib.Path):\n",
    "        return lambda x: pathlib.Path(x).expanduser()\n",
    "    return type(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_object_shape(rgb_img_batch, depth_img_batch, camera_intrinsic_mat, img_width=96, img_height=96):\n",
    "    \"\"\"\n",
    "    Estimates the 3d point cloud shape of each object in the scene. \n",
    "    Returns a list of shape [N, ]\n",
    "    \"\"\"\n",
    "\n",
    "    # demo_data_path = pathlib.Path(\"../../SceneGrasp/outreach/demo_data\")\n",
    "    # data_generator = self.get_demo_data_generator(demo_data_path)\n",
    "\n",
    "    # TOP_K = 200  # TODO: use greedy-nms for top-k to get better distributions!\n",
    "    # all_gripper_vis = []\n",
    "    # for pred_idx in range(pred_dp.get_len()):\n",
    "    #     (pred_grasp_poses_cam_final, pred_grasp_widths, _,) = get_final_grasps_from_predictions_np(\n",
    "    #         pred_dp.scale_matrices[pred_idx][0, 0],\n",
    "    #         pred_dp.endpoints,\n",
    "    #         pred_idx,\n",
    "    #         pred_dp.pose_matrices[pred_idx],\n",
    "    #         TOP_K=TOP_K,\n",
    "    #     )\n",
    "    #     # print(pred_grasp_poses_cam_final.shape)    # (N=124/200/164, 4, 4)\n",
    "    #     # print(pred_grasp_widths.shape)    # (N,)\n",
    "\n",
    "    #     grasp_colors = np.ones((len(pred_grasp_widths), 3)) * [1, 0, 0]    # (N, 3)\n",
    "    #     all_gripper_vis += [get_grasp_vis(pred_grasp_poses_cam_final, pred_grasp_widths, grasp_colors)]\n",
    "\n",
    "    # pred_pcls = pred_dp.get_camera_frame_pcls()    # list containing the pcd of each object in the image. Each element is np array of shape (2048, 3)\n",
    "    # pred_pcls_o3d = []\n",
    "    # for pred_pcl in pred_pcls:\n",
    "    #     pred_pcls_o3d.append(get_o3d_pcd_from_np(pred_pcl))    # convert numpy pcd to open3d pcd\n",
    "    # o3d_pcl = convert_realsense_rgb_depth_to_o3d_pcl(rgb, depth / 1000, self._camera_intrinsic_mat)    # Pointcloud with T (variable) points\n",
    "\n",
    "        # print(\">Showing predicted shapes:\")\n",
    "        # print(\"stage:0\", o3d_pcl)\n",
    "        # print(\"stage:1\", pred_pcls_o3d)\n",
    "        # print(\"stage:2\", all_gripper_vis)\n",
    "\n",
    "    print(\"Wanna REACH HERE\")\n",
    "    pdb.set_trace()\n",
    "    \n",
    "    return o3d_pcl, pred_pcls_o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img):\n",
    "    # Display the image\n",
    "    # cv2.imshow('Image', img)\n",
    "\n",
    "    # # Wait for a key press and then close the window\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    display(img)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_config():\n",
    "    config = AttrDict()\n",
    "\n",
    "    config.env_name = \"Lift\"    # [PickPlaceBread, PickPlaceMilk, PickPlaceCereal, Lift, Reach]\n",
    "    config.robots = \"Panda\"     # \"Sawyer\", \"UR5e\"\n",
    "    config.controller = \"OSC_POSE\"\n",
    "    if config.env_name.startswith(\"PickPlace\"):\n",
    "        config.horizon = 250\n",
    "    elif config.env_name == \"Lift\" or config.env_name == \"Reach\":\n",
    "        config.horizon = 100\n",
    "    else:\n",
    "        config.horizon = 250\n",
    "\n",
    "    config.camera_names = [\"frontview\"]\n",
    "    # config.camera_names = [\"frontview\", \"agentview\", \"robot0_eye_in_hand\"]\n",
    "\n",
    "    config.use_camera_obs = True\n",
    "    config.use_depth_obs = True\n",
    "    config.use_object_obs = True\n",
    "    config.use_proprio_obs = True\n",
    "    config.use_touch_obs = True\n",
    "    config.use_tactile_obs = False\n",
    "    config.use_shape_obs = False\n",
    "\n",
    "    config.img_width = 256\n",
    "    config.img_height = 256\n",
    "\n",
    "    config.expert_dir_timestamp = datetime(2024, 3, 25, 16, 20, 24).strftime('%Y%m%dT%H%M%S')\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created\n",
      "0 Var: robot0_joint_pos_cos, shape: (7,), range: (-0.984718328481616, 0.9999996408110812)\n",
      "1 Var: robot0_joint_pos_sin, shape: (7,), range: (-0.48398395104917447, 0.7069664825132252)\n",
      "2 Var: robot0_joint_vel, shape: (7,), range: (0.0, 0.0)\n",
      "3 Var: robot0_eef_pos, shape: (3,), range: (-0.11390058656385811, 0.9988640022208465)\n",
      "4 Var: robot0_eef_quat, shape: (4,), range: (-0.033086587336056024, 0.997844118038471)\n",
      "5 Var: robot0_gripper_qpos, shape: (2,), range: (-0.020833, 0.020833)\n",
      "6 Var: robot0_gripper_qvel, shape: (2,), range: (0.0, 0.0)\n",
      "7 Var: frontview_image, shape: (256, 256, 3), range: (6, 255)\n",
      "8 Var: frontview_depth, shape: (256, 256, 1), range: (0.9920311570167542, 0.9972583055496216)\n",
      "9 Var: robot0_touch, shape: (2,), range: (0.0, 0.0)\n",
      "10 Var: cube_pos, shape: (3,), range: (0.013955491168617565, 0.8308280673292359)\n",
      "11 Var: cube_quat, shape: (4,), range: (-0.7262194389151002, 0.687462963760112)\n",
      "12 Var: cube_to_robot0_eef_pos, shape: (3,), range: (-0.13774058200754033, 0.16803593489161062)\n",
      "13 Var: cube_to_robot0_eef_quat, shape: (4,), range: (0.034618161618709564, 0.7473995685577393)\n",
      "14 Var: robot0_eef_to_cube_yaw, shape: (1,), range: (0.12196521722764533, 0.12196521722764533)\n",
      "15 Var: robot0_proprio-state, shape: (32,), range: (-0.984718328481616, 0.9999996408110812)\n",
      "16 Var: robot0_touch-state, shape: (2,), range: (0.0, 0.0)\n",
      "17 Var: object-state, shape: (15,), range: (-0.7262194389151002, 0.8308280673292359)\n",
      "Expert directory: ../expert_data/robosuite/lift/panda/osc_pose/20240325T162024-100\n"
     ]
    }
   ],
   "source": [
    "config = define_config()\n",
    "env = create_env(config, verbose=True)\n",
    "\n",
    "# load the camera intrinsic matrix\n",
    "camera_k = {}\n",
    "if isinstance(config.camera_names, str):\n",
    "    camera_k[config.camera_names] = cam_utils.get_camera_intrinsic_matrix(env.sim, config.camera_names, config.img_height, config.img_width)\n",
    "elif isinstance(config.camera_names, list):\n",
    "    for camera_name in config.camera_names:\n",
    "        camera_k[camera_name] = cam_utils.get_camera_intrinsic_matrix(env.sim, camera_name, config.img_height, config.img_width)\n",
    "else:\n",
    "    raise ValueError(\"Incorrect config.camera_names\")\n",
    "\n",
    "# if config.env_name == \"Lift\":\n",
    "#     policy = LiftPolicy(env)\n",
    "# elif config.env_name == \"Reach\":\n",
    "#     policy = ReachPolicy(env)\n",
    "# elif config.env_name.startswith(\"PickPlace\"):\n",
    "#     policy = PickPlacePolicy(env)\n",
    "# print(\"Policy object created\")\n",
    "\n",
    "timestamp = config.expert_dir_timestamp\n",
    "expert_dir = pathlib.Path(f\"../expert_data/robosuite/{config.env_name.lower()}/{config.robots.lower()}/{config.controller.lower()}/{timestamp}-{config.horizon}/\")\n",
    "print(\"Expert directory:\", expert_dir)\n",
    "\n",
    "episodes = load_episodes(expert_dir / \"episodes/\")\n",
    "episodes = [val for _, val in episodes.items()]\n",
    "num_eps = len(episodes)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved checkpoint:  ../../SceneGrasp/checkpoints/scene_grasp.ckpt\n",
      "Using model class from: /Users/saqib/Desktop/ERLab/robot_manipulation/SceneGrasp/simnet/lib/net/models/panoptic_net.py\n",
      "Restoring from checkpoint: ../../SceneGrasp/checkpoints/scene_grasp.ckpt\n"
     ]
    }
   ],
   "source": [
    "hparams = get_scene_grasp_model_params()\n",
    "print(\"Loading model from saved checkpoint: \", hparams.checkpoint)\n",
    "scene_grasp_model = SceneGraspModel(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Over episodes:   4%|▍         | 1/24 [00:04<01:38,  4.28s/it]/Users/saqib/Desktop/ERLab/robot_manipulation/SceneGrasp/scene_grasp/scale_shape_grasp_ae/model/auto_encoder_scale_grasp.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  endpoints[\"grasp_width_one_hot\"] = F.softmax(out_pc[:, :, 4:14])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Showing predicted shapes:\n",
      "FEngine (64 bits) created at 0x3b4650000 (threading is enabled)\n",
      "FEngine resolved backend: OpenGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[error] GLFW error: Cocoa: Failed to find service port for display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Showing predicted grasps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Over episodes:   8%|▊         | 2/24 [00:51<10:55, 29.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Showing predicted shapes:\n",
      ">Showing predicted grasps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Over episodes:  12%|█▎        | 3/24 [00:55<06:14, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Showing predicted shapes:\n",
      ">Showing predicted grasps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Over episodes:  17%|█▋        | 4/24 [00:57<03:53, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Showing predicted shapes:\n",
      ">Showing predicted grasps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Over episodes:  21%|██        | 5/24 [00:59<02:33,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Showing predicted shapes:\n",
      ">Showing predicted grasps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Over episodes:  25%|██▌       | 6/24 [01:05<02:15,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Showing predicted shapes:\n",
      ">Showing predicted grasps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Over episodes:  29%|██▉       | 7/24 [01:08<01:37,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Showing predicted shapes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Over episodes:  29%|██▉       | 7/24 [01:11<02:53, 10.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m o3d_pcl \u001b[38;5;241m=\u001b[39m convert_realsense_rgb_depth_to_o3d_pcl(rgb_img, depth_img \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m, cam_intrinsic_mat)    \u001b[38;5;66;03m# Pointcloud with T (variable) points\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>Showing predicted shapes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mo3d_pcl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpred_pcls_o3d\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>Showing predicted grasps:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m o3d\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mdraw(pred_pcls_o3d \u001b[38;5;241m+\u001b[39m all_gripper_vis)  \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/rlenv/lib/python3.9/site-packages/open3d/visualization/draw.py:112\u001b[0m, in \u001b[0;36mdraw\u001b[0;34m(geometry, title, width, height, actions, lookat, eye, up, field_of_view, intrinsic_matrix, extrinsic_matrix, bg_color, bg_image, ibl, ibl_intensity, show_skybox, show_ui, raw_mode, point_size, line_width, animation_time_step, animation_duration, rpc_interface, on_init, on_animation_frame, on_animation_tick, non_blocking_and_return_uid)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w\u001b[38;5;241m.\u001b[39muid\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mgui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mApplication\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "rgb_cam_names = [key for key in episodes[0].keys() if \"_image\" in key]\n",
    "depth_cam_names = [key for key in episodes[0].keys() if \"_depth\" in key]\n",
    "\n",
    "if isinstance(config.camera_names, str):\n",
    "    rgb_cam_names = [cam_name for cam_name in rgb_cam_names if config.camera_names in cam_name]\n",
    "    depth_cam_names = [cam_name for cam_name in depth_cam_names if config.camera_names in cam_name]\n",
    "elif isinstance(config.camera_names, list):\n",
    "    rgb_temp = [cam_name + \"_image\" for cam_name in config.camera_names]\n",
    "    depth_temp = [cam_name + \"_depth\" for cam_name in config.camera_names] \n",
    "    rgb_cam_names = list(set(rgb_temp).intersection(set(rgb_cam_names)))\n",
    "    depth_cam_names = list(set(depth_temp).intersection(set(depth_cam_names)))\n",
    "\n",
    "num_imgs_success = 0\n",
    "num_imgs_fail = 0\n",
    "\n",
    "for episode in tqdm(episodes, desc=\"Over episodes\"):\n",
    "    for rgb_cam_name, depth_cam_name in zip(rgb_cam_names, depth_cam_names):\n",
    "        rgb_imgs = episode[rgb_cam_name]    # numpy array\n",
    "        depth_imgs = episode[depth_cam_name].squeeze()    # numpy array\n",
    "        num_imgs = rgb_imgs.shape[0]\n",
    "        cam_intrinsic_mat = camera_k[rgb_cam_name.rsplit(\"_image\", 1)[0]]\n",
    "\n",
    "        for rgb_img, depth_img in zip(rgb_imgs, depth_imgs):\n",
    "            # rgb = cv2.resize(rgb_img_batch[idx], (img_width, img_height))\n",
    "            # depth = cv2.resize(depth_img_batch[idx], (img_width, img_height))\n",
    "            # rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)    # Convert the image from BGR to RGB (cv2 reads images in BGR format)\n",
    "            \n",
    "#             rgb_img_pil = Image.fromarray(rgb_img)    # Convert the image to a PIL Image object\n",
    "#             depth_img_pil = Image.fromarray(depth_img)\n",
    "#             # display(rgb_img_pil)\n",
    "#             # display(depth_img_pil)\n",
    "#             # rgb_img_pil.show()\n",
    "#             # depth_img_pil.show()\n",
    "\n",
    "            pred_dp = scene_grasp_model.get_predictions(rgb_img, depth_img, cam_intrinsic_mat)\n",
    "            if pred_dp is None:\n",
    "                num_imgs_fail += 1\n",
    "            else:\n",
    "                num_imgs_success += 1\n",
    "                all_gripper_vis = []\n",
    "                for pred_idx in range(pred_dp.get_len()):\n",
    "                    pred_grasp_poses_cam_final, pred_grasp_widths, _, = get_final_grasps_from_predictions_np(pred_dp.scale_matrices[pred_idx][0, 0],\n",
    "                                                                                                            pred_dp.endpoints,\n",
    "                                                                                                            pred_idx,\n",
    "                                                                                                            pred_dp.pose_matrices[pred_idx],\n",
    "                                                                                                            TOP_K=200,)\n",
    "                    grasp_colors = np.ones((len(pred_grasp_widths), 3)) * [1, 0, 0]    # (N, 3)\n",
    "                    all_gripper_vis += [get_grasp_vis(pred_grasp_poses_cam_final, pred_grasp_widths, grasp_colors)]\n",
    "                \n",
    "                pred_pcls = pred_dp.get_camera_frame_pcls()    # list containing the pcd of each object in the image. Each element is np array of shape (2048, 3)\n",
    "                pred_pcls_o3d = []\n",
    "                for pred_pcl in pred_pcls:\n",
    "                    pred_pcls_o3d.append(get_o3d_pcd_from_np(pred_pcl))    # convert numpy pcd to open3d pcd\n",
    "                o3d_pcl = convert_realsense_rgb_depth_to_o3d_pcl(rgb_img, depth_img / 1000, cam_intrinsic_mat)    # Pointcloud with T (variable) points\n",
    "\n",
    "                print(\">Showing predicted shapes:\")\n",
    "                o3d.visualization.draw([o3d_pcl] + pred_pcls_o3d)  # type:ignore\n",
    "                \n",
    "                print(\">Showing predicted grasps:\")\n",
    "                o3d.visualization.draw(pred_pcls_o3d + all_gripper_vis)  # type:ignore\n",
    "                break\n",
    "\n",
    "print(f\"Success ratio: {num_imgs_success / (num_imgs_success + num_imgs_fail)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[204, 192, 183],\n",
       "         [206, 194, 183],\n",
       "         [201, 191, 181],\n",
       "         ...,\n",
       "         [200, 192, 183],\n",
       "         [202, 194, 185],\n",
       "         [203, 195, 186]],\n",
       "\n",
       "        [[199, 189, 180],\n",
       "         [204, 193, 184],\n",
       "         [203, 192, 183],\n",
       "         ...,\n",
       "         [208, 200, 191],\n",
       "         [208, 200, 191],\n",
       "         [209, 201, 192]],\n",
       "\n",
       "        [[196, 187, 179],\n",
       "         [204, 194, 184],\n",
       "         [207, 195, 186],\n",
       "         ...,\n",
       "         [189, 180, 171],\n",
       "         [187, 178, 169],\n",
       "         [195, 186, 177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 109],\n",
       "         [115, 113, 109],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109]],\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 108]],\n",
       "\n",
       "        [[114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [113, 111, 108],\n",
       "         [113, 112, 108],\n",
       "         [114, 112, 109]]],\n",
       "\n",
       "\n",
       "       [[[204, 192, 183],\n",
       "         [206, 194, 183],\n",
       "         [201, 191, 181],\n",
       "         ...,\n",
       "         [200, 192, 183],\n",
       "         [202, 194, 185],\n",
       "         [203, 195, 186]],\n",
       "\n",
       "        [[199, 189, 180],\n",
       "         [204, 193, 184],\n",
       "         [203, 192, 183],\n",
       "         ...,\n",
       "         [208, 200, 191],\n",
       "         [208, 200, 191],\n",
       "         [209, 201, 192]],\n",
       "\n",
       "        [[196, 187, 179],\n",
       "         [204, 194, 184],\n",
       "         [207, 195, 186],\n",
       "         ...,\n",
       "         [189, 180, 171],\n",
       "         [187, 178, 169],\n",
       "         [195, 186, 177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 109],\n",
       "         [115, 113, 109],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109]],\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 108]],\n",
       "\n",
       "        [[114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [113, 111, 108],\n",
       "         [113, 112, 108],\n",
       "         [114, 112, 109]]],\n",
       "\n",
       "\n",
       "       [[[204, 192, 183],\n",
       "         [206, 194, 183],\n",
       "         [201, 191, 181],\n",
       "         ...,\n",
       "         [200, 192, 183],\n",
       "         [202, 194, 185],\n",
       "         [203, 195, 186]],\n",
       "\n",
       "        [[199, 189, 180],\n",
       "         [204, 193, 184],\n",
       "         [203, 192, 183],\n",
       "         ...,\n",
       "         [208, 200, 191],\n",
       "         [208, 200, 191],\n",
       "         [209, 201, 192]],\n",
       "\n",
       "        [[196, 187, 179],\n",
       "         [204, 194, 184],\n",
       "         [207, 195, 186],\n",
       "         ...,\n",
       "         [189, 180, 171],\n",
       "         [187, 178, 169],\n",
       "         [195, 186, 177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 109],\n",
       "         [115, 113, 109],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109]],\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 108]],\n",
       "\n",
       "        [[114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [113, 111, 108],\n",
       "         [113, 112, 108],\n",
       "         [114, 112, 109]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[204, 192, 183],\n",
       "         [206, 194, 183],\n",
       "         [201, 191, 181],\n",
       "         ...,\n",
       "         [200, 192, 183],\n",
       "         [202, 194, 185],\n",
       "         [203, 195, 186]],\n",
       "\n",
       "        [[199, 189, 180],\n",
       "         [204, 193, 184],\n",
       "         [203, 192, 183],\n",
       "         ...,\n",
       "         [208, 200, 191],\n",
       "         [208, 200, 191],\n",
       "         [209, 201, 192]],\n",
       "\n",
       "        [[196, 187, 179],\n",
       "         [204, 194, 184],\n",
       "         [207, 195, 186],\n",
       "         ...,\n",
       "         [189, 180, 171],\n",
       "         [187, 178, 169],\n",
       "         [195, 186, 177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 109],\n",
       "         [115, 113, 109],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109]],\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 108]],\n",
       "\n",
       "        [[114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [113, 111, 108],\n",
       "         [113, 112, 108],\n",
       "         [114, 112, 109]]],\n",
       "\n",
       "\n",
       "       [[[204, 192, 183],\n",
       "         [206, 194, 183],\n",
       "         [201, 191, 181],\n",
       "         ...,\n",
       "         [200, 192, 183],\n",
       "         [202, 194, 185],\n",
       "         [203, 195, 186]],\n",
       "\n",
       "        [[199, 189, 180],\n",
       "         [204, 193, 184],\n",
       "         [203, 192, 183],\n",
       "         ...,\n",
       "         [208, 200, 191],\n",
       "         [208, 200, 191],\n",
       "         [209, 201, 192]],\n",
       "\n",
       "        [[196, 187, 179],\n",
       "         [204, 194, 184],\n",
       "         [207, 195, 186],\n",
       "         ...,\n",
       "         [189, 180, 171],\n",
       "         [187, 178, 169],\n",
       "         [195, 186, 177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 109],\n",
       "         [115, 113, 109],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109]],\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 108]],\n",
       "\n",
       "        [[114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [113, 111, 108],\n",
       "         [113, 112, 108],\n",
       "         [114, 112, 109]]],\n",
       "\n",
       "\n",
       "       [[[204, 192, 183],\n",
       "         [206, 194, 183],\n",
       "         [201, 191, 181],\n",
       "         ...,\n",
       "         [200, 192, 183],\n",
       "         [202, 194, 185],\n",
       "         [203, 195, 186]],\n",
       "\n",
       "        [[199, 189, 180],\n",
       "         [204, 193, 184],\n",
       "         [203, 192, 183],\n",
       "         ...,\n",
       "         [208, 200, 191],\n",
       "         [208, 200, 191],\n",
       "         [209, 201, 192]],\n",
       "\n",
       "        [[196, 187, 179],\n",
       "         [204, 194, 184],\n",
       "         [207, 195, 186],\n",
       "         ...,\n",
       "         [189, 180, 171],\n",
       "         [187, 178, 169],\n",
       "         [195, 186, 177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 109],\n",
       "         [115, 113, 109],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109]],\n",
       "\n",
       "        [[115, 113, 109],\n",
       "         [114, 113, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 109],\n",
       "         [114, 112, 108]],\n",
       "\n",
       "        [[114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         [114, 112, 108],\n",
       "         ...,\n",
       "         [113, 111, 108],\n",
       "         [113, 112, 108],\n",
       "         [114, 112, 109]]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99684495"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_img_batch.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../expert_data/robosuite_expert/PickPlaceBread/Panda/OSC_POSE/20230708T191838-agentview-250'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.path.dirname(\"../expert_data/robosuite_expert/PickPlaceBread/Panda/OSC_POSE/20230708T191838-agentview-250/expert_data/\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
